---
title: "Regression modelling"
subtitle: ENVX2001 Applied Statistical Methods
author: Liana Pozza
institute: The University of Sydney
date: last-modified # today | last-modified
date-format: "MMM YYYY"
execute:
  cache: false
  echo: true
editor-options:
  canonical: true
toc: true
toc-depth: 1
toc-title: Outline
chalkboard: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  cache = TRUE)
library(tidyverse)
ggplot2::theme_set(cowplot::theme_half_open())
# ggplot2::theme_set(ggplot2::theme_minimal())
```

# Welcome to regression modelling!

## About me

::: {layout-ncol=2}
- Lecturer in Agricultural Data Science
- Spatial modelling and mapping, Soil science, Precision Agriculture
- This year back to being a student again, GradCert (Higher Education)

![Narrabri cotton field](assets/Pozza_natural_habitat.jpg)
:::


## Learning Outcomes {auto-animate=true}

LO1. demonstrate proficiency in designing sample schemes and analysing data from them using using R

LO2. describe and identify the basic features of an experimental design; replicate, treatment structure and blocking structure

LO3. demonstrate proficiency in the use or the statistical programming language R to an ANOVA and fit regression models to experimental data

LO4. demonstrate proficiency in the use or the statistical programming language R to use multivariate methods to find patterns in data

LO5. interpret the output and understand conceptually how its derived of a regression, ANOVA and multivariate analysis that have been calculated by R

LO6. write statistical and modelling results as part of a scientific report

LO7. appraise the validity of statistical analyses used publications.


## Learning Outcomes {auto-animate=true}

LO1. demonstrate proficiency in designing sample schemes and analysing data from them using using R

LO2. describe and identify the basic features of an experimental design; replicate, treatment structure and blocking structure

**LO3. demonstrate proficiency in the use or the statistical programming language R to** an ANOVA and **fit regression models to experimental data**

LO4. demonstrate proficiency in the use or the statistical programming language R to use multivariate methods to find patterns in data

**LO5. interpret the output and understand conceptually how its derived of a regression**, ANOVA and multivariate analysis that have been calculated by R

LO6. write statistical and modelling results as part of a scientific report

LO7. appraise the validity of statistical analyses used publications.

<!---
therefore, want students how to fit a linear regression model and be able to interpret 
- Want to make sure we are using it for the right thing (linear relationship)
- want to make sure we are fitting it properly
- Want to then be able to interpret the output
--->

## Workflow {auto-animate=true}

1. Model development
    + Explore: visualise, summarise
    + Transform predictors: linearise, reduce skewness/leverage
    + Model: fit, check assumptions, interpret, transform. Repeat.
  
2. Variable selection
    + VIF: remove predictors with high variance inflation factor
    + Model selection: stepwise selection, AIC, principle of parsimony, assumption checks
  
3. Predictive modelling
    + Predict: Use the model to predict new data
    + Validate: Evaluate the modelâ€™s performance


## Workflow {auto-animate=true}

1. Model development
    + Explore: visualise, summarise
    + Transform predictors: linearise, reduce skewness/leverage
    + Model: fit, check assumptions, interpret, transform. Repeat.


# Brief history

![Adrien-Marie Legendre](assets/legendre.jpg)
![Carl Friedrich Gauss](assets/gauss.jpg)
![Francis Galton](assets/galton.jpg)

Adrien-Marie Legendre, Carl Friedrich Gauss & Francis Galton




## Least squares, correlation and astronomy

- Method of least squares first [**published**]{style="color: firebrick"} paper by Adrien-Marie Legendre in 1805
- Technique of least squares used by Carl Friedrich Gauss in 1809 to fit a parabola to the orbit of the asteroid Ceres
- Model fitting first [**published**]{style="color: firebrick"} by Francis Galton in 1886 to the problem of predicting the height of a child from the height of the parents

:::{.callout-note}
Many other people contributed to the development of regression analysis, but these three are the "most" well-known.
:::

## Galton's data 

```{r}
library(HistData)
dplyr::tibble(Galton)
```

- 928 children of 205 pairs of parents
- Height of parents and children measured in inches
- Size classes were binned (hence data looks discrete)


*Galton, F. (1886). Regression Towards Mediocrity in Hereditary Stature Journal of the Anthropological Institute, 15, 246-263*

---

```{r}
library(ggplot2)
ggplot(Galton, aes(x = parent, y = child)) +
  geom_point(alpha = .2, size = 3)
```

---

```{r}
library(ggplot2)
ggplot(Galton, aes(x = parent, y = child)) +
  geom_point(alpha = .2, size = 3) + geom_smooth(method = "lm")
```

## Regression modelling in R

. . .

```{r}
fit <- lm(child ~ parent, data = Galton)
summary(fit)
```

<br> 

That's it... you have fitted a model...**But how do we assess its quality**?


<!-- ```{r}
library(report)
report(fit) 
```  -->

# Simple linear regression

## Defining a linear relationship {.nostretch}

-  Pearson correlation coefficient measures the linear correlation between two variables
- Does not distinguish different *patterns* of association, only the *strength* of the association

![](assets/correlation.png)

- Not quite usable for *predictive* modelling, or for *inference* about the relationship between variables


## Anscombe's quartet

```{r}
#| code-fold: true
library(tidyverse)
anscombe %>%
  pivot_longer(everything(), cols_vary = "slowest",
    names_to = c(".value", "set"), names_pattern = "(.)(.)") %>%
  ggplot(aes(x = x, y = y)) +
    geom_point(size = 3) +
    geom_smooth(method = "lm", se = FALSE) +
    facet_wrap(~set, ncol = 4)
```

*All of these data have a correlation coefficient of about 0.8, but **only one** of them meet the assumptions of a linear model.*

## Datasaurus Dozen

```{r}
#| code-fold: true
library(datasauRus)
ggplot(datasaurus_dozen, aes(x=x, y=y)) +
  geom_point(size = .5, alpha = .3) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~dataset, ncol = 6)
```

*All of these data have the **same** correlation coefficient, mean and s.d. but look vastly different.*

## Simple linear regression modelling {auto-animate="true"}

<!-- ```{r}
words fall of the page at the moment 
- solved by removing callout box
```  -->

We want to predict an outcome $Y$ based on a predictor $x$ for $i$ number of observations: 

$$Y_i = \color{royalblue}{\beta_0 + \beta_1 x_i} +\color{red}{\epsilon_i}$$

where

$$\epsilon_i \sim N(0, \sigma^2)$$

- $Y_i$, the *response*, is an observed value of the dependent variable.
- $\beta_0$, the *constant*, is the population intercept and is **fixed**.
- $\beta_1$ is the population *slope* parameter, and like $\beta_0$, is also **fixed**.
- $\epsilon_i$ is the error associated with predictions of $y_i$, and unlike $\beta_0$ or $\beta_1$, it is *not fixed*.

We tend to associate $\epsilon_i$ with the **residual**, which is a positive or negative difference from the "predicted" response, rather than error itself which is a difference from the **true** response

## Simple linear regression: in pictures

![](assets/Blank_plot_Galton.png)
<!-- 
Aim of this: want to demonstrate the equation in pictures
The goal of obtaining this data was to model the relationship between 
parent and child height -> is there a direct link?
- for each parent height, a child height is recorded
- this is how we get the points
- then want to fit a model, i.e. draw a line. 
- draw line so it goes through as many points as possible
- This line allows us to understand the strength of the relationship, 
describe it and predict new values of y if we wanted. 
- WE can obtain parameters from the line to do this
-> B0 = intercept
-> b1 = slope
- residuals = error between what has actually been observed and what the line has 'predicted', i.e. what it has described the relationship to be
    - take this into account when predicting new values of y
- so if we want to predict new values of y, we follow the line and therefore 
can sub into the equation. " i.e. if a parent is this tall, how tall might the child be?"

process - draw line, indicate eqtn params

-
 -->

## Interpreting the relationship {auto-animate="true"}

$$Y_i = \color{royalblue}{\beta_0 + \beta_1 x_i} +\color{red}{\epsilon_i}$$

[Basically, a *deterministic* straight line equation $y = c + mx$, with added *random* variation that is normally distributed]{style="color: seagreen;"}

. . .

- Response = [Prediction]{style="color: royalblue"} + [Error]{style="color: red"}
- Response = [Signal]{style="color: royalblue"} + [Noise]{style="color: red"}
- Response = [Model]{style="color: royalblue"} + [Unexplained]{style="color: red"}
- Response = [Deterministic]{style="color: royalblue"} + [Random]{style="color: red"}
- Response = [Explainable]{style="color: royalblue"} + [Everything else]{style="color: red"}




## Fitting the model {auto-animate="true"}

- The *residual* is the difference between the observed value of the response and the predicted value:

$$\hat\epsilon_i = y_i - \color{royalblue}{\hat{y}_i}$$

. . .

where $\color{royalblue}{\hat{y}_i}$ is the predicted value of $y_i$:

$$\color{royalblue}{\hat{y}_i} = \beta_0 + \beta_1 x_i$$

. . .

therefore:

$$\hat\epsilon_i = y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)}$$

. . .

- We use the **method of least squares** and minimise the sum of the squared residuals (SSR):

$$\sum_{i=1}^n \hat\epsilon_i^2 = \sum_{i=1}^n (y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)})^2$$

## {auto-animate="true"}

$$\sum_{i=1}^n \hat\epsilon_i^2 = \sum_{i=1}^n (y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)})^2$$

. . .

Finding the minimum SSR requires solving the following problem:

$$\color{firebrick}{argmin_{\beta_0, \beta_1}} \sum_{i=1}^n (y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)})^2$$

## {auto-animate="true"}

$$\color{firebrick}{argmin_{\beta_0, \beta_1}} \sum_{i=1}^n (y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)})^2$$


![[source](https://github.com/Enchufa2/ls-springs)](assets/leastsquares.gif){fig-align="center"}


## Using `lm()`

Linear regression in R is performed using the `lm()` function:

```{r}
fit <- lm(child ~ parent, data = Galton)
fit
```

. . .

<br>

Does the input of `lm()` look familiar? It should!

```{r}
#| eval: false
model <- aov(y ~ x, data)
```
## Handling `lm()` output

. . .

```{r}
names(fit)
```

. . .

We can extract the output objects using the `$` operator:

```{r}
fit$coefficients
```

```{r}
fit$call
```

## Using external packages to handle `lm()` output

. . .

The `broom` package simplifies handling of model objects by converting them into tidy data frames:
```{r}
library(broom)
summ_fit <- tidy(fit)
summ_fit
```

. . .

The `sjPlot` package is useful to create a summary table:
```{r}
library(sjPlot)
sjPlot::tab_model(fit, dv.labels = "")
```

---

The `glance` function is useful for quickly assessing model parameters:

```{r}
broom::glance(fit)
```

. . .

The `augment` function adds the fitted values and residuals to the data frame:

```{r}
augment(fit)
```

# Assessing model fit

## Assumptions

The data **must** meet certain criteria, which we often call *assumptions*. They can be remembered using **LINE**:

- **L**inearity. The relationship between $y$ and $x$ is linear.
- **I**ndependence. The errors $\epsilon$ are independent.
- **N**ormal. The errors $\epsilon$ are normally distributed.
- **E**qual Variance. At each value of $x$, the variance of $y$ is the same i.e. homoskedasticity, or constant variance.

. . .

**Notice any similarities to the assumptions of ANOVA?**

:::{.callout-tip}
All but the independence assumption can be assessed using diagnostic plots. 
:::

## Assumptions with `plot()`

```{r}
#| code-fold: true
par(mfrow= c(2, 2))
plot(fit)
```


## Assumptions using `performance`

```{r}
library(performance)
```

> With great power....

. . .

```{r}
#| eval: false
performance::check_model(fit) # check all assumptions

# check specific assumption(s)
performance::check_model(fit, check = "xxx") 
```

:::{.callout-tip}
It might be easier to specify the assumptions using the `check` argument, as the default method might use diagnostic plots that are not always easy to interpret.
:::

## Assumption: Linearity

Prior knowledge and visual inspection comes into play. Does the relationship look approximately linear?

```{r}
#| code-fold: true
ggplot(Galton, aes(x = parent, y = child)) +
  geom_point(alpha = .2, size = 3) +
  # labs(
  #   x = expression("Temperature " ( degree~C)), 
  #   y = "Ozone (parts per billion)") +
  geom_smooth(method = "lm", se = FALSE)
```

:::{.callout-tip}
After running the regression, the linearity assumption can be checked *again* by looking at a plot of the residuals against $x$ i.e. size.
:::

---

```{r}
performance::check_model(fit, check = "linearity")
```

- Where the reference line is above 0, the model *underestimates* size, and where it is below 0, it *overestimates* size.
- If the linearity assumption is **violated**, there is no reason to validate the model since it is no longer suitable for the data.


## Assumption: Independence

This assumption is addressed during experimental design, but issues like correlation between errors and patterns occurring due to time are possible

- Randomisation and proper sampling handles *most* issues with independence
- Violations may occur if observations of the same subject are related i.e. [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)
- Violations may occur in time-series data, if the same subjects are sampled i.e. [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation)

## Assumption: Normality

![](assets/residual.jpg)

---

```{r}
performance::check_model(fit, check = c("normality", "qq"))
```

---

## Assessing normality using residuals

- **Light-tailed**: small variance in residuals, resulting in a narrow distribution
- **Heavy-tailed**: many extreme positive and negative residuals, resulting in a wide distribution
- **Left-skewed** (n shape): more data falls to the left of the mean
- **Right-skewed** (u shape): more data falls to the right of the mean

---

```{r} 
#| code-fold: true
set.seed(915)
x <- rnorm(100)
y <- 2 + 5 * x + rchisq(100, df = 2)
df <- data.frame(x, y)
performance::check_model(lm(y ~ x, data = df),
  check = c(c("normality", "qq")))
```

. . .

[Heavy-tailed]{.absolute top=200 left=320}
[Right-skewed]{.absolute top=340 left=200}

[Heavy-tailed]{.absolute bottom=200 right=0}
[Right-skewed]{.absolute bottom=300 right=200}

---

```{r} 
#| code-fold: true
set.seed(1028)
x <- rnorm(100)
y <- 2 + 5 * x + rchisq(100, df = 3) * -1
df <- data.frame(x, y)
performance::check_model(lm(y ~ x, data = df),
  check = c(c("normality", "qq")))
```

. . .

[Heavy-tailed]{.absolute bottom=170 left=120}
[Left-skewed]{.absolute bottom=350 left=250}

[Heavy-tailed]{.absolute bottom=200 right=380}
[Left-skewed]{.absolute bottom=430 right=200}

---

```{r} 
#| code-fold: true
set.seed(1028)
x <- rnorm(100)
y <- 2 + 5 * x + rnbinom(100, 10, .5)
df <- data.frame(x, y)
performance::check_model(lm(y ~ x, data = df),
  check = c(c("normality", "qq")))
```

. . .

[Light-tailed?]{.absolute bottom=200 left=50}
[Right-skewed?]{.absolute top=320 left=180}
[Outlier?]{.absolute top=120 left=400}

[Light-tailed]{.absolute bottom=200 right=320}
[Outlier?]{.absolute bottom=150 right=0}

## External resources on QQ plots

- [How to interpret a QQ plot](https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot)
- [QQ plot interpretation](https://math.illinois.edu/system/files/inline-files/Proj9AY1516-report2.pdf)

## Asumption: Equal variances

```{r}
performance::check_model(fit, check = c("homogeneity", "outliers"))
```

## What is a standardised residual?

- The standardised residual is the residual divided by the standard error of the residual.

$$Standardised\ residual = \frac{Residual}{Standard\ error\ of\ the\ residual}$$

- Number of standard deviations that the residual is from the *mean* of the residuals.
- Makes it easy to assess the **equal variances** assumption (among other things).
  

# Summing up
::: {.fragment}
- we fit a linear model to represent a linear relationship between two variables
    + used method of least squares to find the best fitting line
:::
::: {.fragment}
- Model equation; $Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$
    + response ($Y_i$) = observed value of the dependent variable.
    + constant ($\beta_0$) = population intercept
    + slope ($\beta_1$) = population slope parameter
    + error ($\epsilon_i$) = error associated with predictions of y
        - error = Residuals = observed - predicted
:::  
::: {.fragment}
- Check assumptions to understand the validity of the model
    + linearity, independence, normality, equal variance (LINE)
:::
  

# Next lecture: Continuing with regression modelling


# Thanks!

**Questions? Comments?**

Slides made with [Quarto](https://quarto.org)
